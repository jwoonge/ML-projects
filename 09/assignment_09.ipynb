{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnRrNo7aebkOBD9YY4wxps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwoonge/ML-projects/blob/master/09/assignment_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqqTw_oX1ljt"
      },
      "source": [
        "# Classification for Multiple Categories using Pytorch\n",
        "20141261 송제웅  \n",
        "<hr>  \n",
        "\n",
        "## 0. Import library\n",
        "---\n",
        "import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylPDAp1r0pNn"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08y3CH6Y133C"
      },
      "source": [
        "# 1. DownLoad and Normalize dataset (MNIST)\n",
        "---  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXZSofdf05-r"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),\n",
        "])\n",
        "\n",
        "data_path = './MNIST'\n",
        "data_train = datasets.MNIST(root = data_path, train = True, download = True, transform = transform)\n",
        "data_test = datasets.MNIST(root = data_path, train = False, download = True, transform = transform)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqOC-JTv2Q_7"
      },
      "source": [
        "## 2. Design NN Model\n",
        "---\n",
        "three fully connected layers with an activation function of sigmoid, LogSoftmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0wW9W8827VS"
      },
      "source": [
        "class classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classification, self).__init__()\n",
        "        self.classifier1 = nn.Sequential(\n",
        "            nn.Linear(in_features=28*28, out_features=20*20),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.classifier2 = nn.Sequential(\n",
        "            nn.Linear(in_features=20*20, out_features=10*10),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.classifier3 = nn.Sequential(\n",
        "            nn.Linear(in_features=10*10, out_features=10),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "        )\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = inputs.view(inputs.size(0), -1)\n",
        "        x = self.classifier1(x)\n",
        "        x = self.classifier2(x)\n",
        "        out = self.classifier3(x)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YArDEad-lgtH"
      },
      "source": [
        "def train(model, data_train, data_train_batch, optimizer, criterion, device='cuda'):\n",
        "    n_batch = 0\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    for batch_idx, (x, y) in enumerate(data_train_batch):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model.forward(x)\n",
        "        loss = criterion(pred, y)\n",
        "        avg_loss += loss.item()\n",
        "        avg_acc += accuracy(pred, y)\n",
        "        n_batch += 1\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss /= n_batch\n",
        "    avg_acc /= n_batch\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "def test(model, x, y, criterion):\n",
        "    pred = model.forward(x)\n",
        "    loss = criterion(pred, y).item()\n",
        "    acc = accuracy(pred, y)\n",
        "    return loss, acc\n",
        "\n",
        "def accuracy(pred, y):\n",
        "    correct_cnt = 0\n",
        "    num_sample = len(y)\n",
        "    for i in range(num_sample):\n",
        "        pred_class = pred[i].argmax()\n",
        "        if y[i]==pred_class:\n",
        "            correct_cnt += 1\n",
        "    return 100 * correct_cnt / num_sample\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik0LrxaapEOl"
      },
      "source": [
        "def learn(model, data_train, data_test, criterion, optimizer, batch_size, epoch, device='cuda'):\n",
        "    data_train_batch = torch.utils.data.DataLoader(data_train, batch_size)\n",
        "    test_x, test_y = data_test.test_data.view((10000,28*28)), data_test.test_labels\n",
        "    test_x, test_y = torch.tensor(test_x, dtype=torch.float, device=device), test_y.to(device)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        loss_test, acc_test = test(model, test_x, test_y, criterion)\n",
        "        loss_train, acc_train = train(model, data_train, data_train_batch, optimizer, criterion, device)\n",
        "        \n",
        "        print(loss_train, acc_train, loss_test, acc_test)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAJOmBkx9vTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9678512-7c12-442c-8030-3b343b1553ec"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if is_cuda else 'cpu')\n",
        "learning_rate = 0.01\n",
        "classifier = classification().to(device)\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learn(classifier, data_train, data_test, criterion, optimizer, 32, 20)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.1599051619847613 33.35666666666667 2.3770768642425537 10.32\n",
            "1.274776882648468 67.37666666666667 1.5204073190689087 61.08\n",
            "0.7396785102208455 80.425 0.8189350366592407 73.32\n",
            "0.5404514979839325 85.63666666666667 0.6273154020309448 80.25\n",
            "0.4460391818881035 88.03 0.5850847363471985 81.97\n",
            "0.3940278320789337 89.195 0.567023754119873 82.69\n",
            "0.3608978033999602 89.94833333333334 0.5439427495002747 83.55\n",
            "0.33715779530803364 90.55666666666667 0.5137438774108887 84.77\n",
            "0.3185874218980471 91.04166666666667 0.47901371121406555 85.88\n",
            "0.30311317221323647 91.36666666666666 0.44540122151374817 87.0\n",
            "0.2896316826224327 91.695 0.4126471281051636 87.86\n",
            "0.27751973872184754 92.02 0.38317134976387024 88.63\n",
            "0.26640949037273726 92.33 0.3580685555934906 89.48\n",
            "0.2560765894611677 92.60333333333334 0.33595722913742065 90.14\n",
            "0.24638047001461189 92.885 0.31646180152893066 90.73\n",
            "0.23723068998654684 93.11333333333333 0.2981724143028259 91.34\n",
            "0.22856724297106265 93.36833333333334 0.282330185174942 91.95\n",
            "0.2203485246459643 93.60833333333333 0.2676621973514557 92.29\n",
            "0.2125437461445729 93.85166666666667 0.25551316142082214 92.68\n",
            "0.20512830230494342 94.07333333333334 0.24405217170715332 93.05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}