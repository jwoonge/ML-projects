{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9LQwz+37ClJ4AWZruy4yd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwoonge/ML-projects/blob/master/11/assignment_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiGnPwiOIU4J"
      },
      "source": [
        "# Convolutional Neural Network for the classification task on MNIST\n",
        "20141261 송제웅\n",
        "<hr>\n",
        "  \n",
        "## 0. Import library\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqB3iaf8Imew"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdyZwE1WIpUP"
      },
      "source": [
        "## 1. Download and Normalize dataset (MNIST)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHcCR3aCItPH"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),\n",
        "])\n",
        "\n",
        "data_path = './MNIST'\n",
        "data_train = datasets.MNIST(root = data_path, train = False, download = True, transform = transform)\n",
        "data_test = datasets.MNIST(root = data_path, train = True, download = True, transform = transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAGO2AXVJVxg"
      },
      "source": [
        "## 2. Design Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ACjgnmJaz4"
      },
      "source": [
        "class CNN_module(nn.Module):\n",
        "    def __init__(self, num_class=10, size_kernel=5):\n",
        "        super(CNN_module, self).__init__()\n",
        "        self.num_class = num_class\n",
        "        self.size_kernel = size_kernel\n",
        "\n",
        "        #### Feature Layer ####\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=self.size_kernel, stride=1, padding=int((size_kernel-1)/2), bias=True)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=self.size_kernel, stride=1, padding=int((size_kernel-1)/2), bias=True)\n",
        "\n",
        "        self.conv_layer1 = nn.Sequential(self.conv1, nn.MaxPool2d(kernel_size=2), nn.ReLU(True))\n",
        "        self.conv_layer2 = nn.Sequential(self.conv2, nn.MaxPool2d(kernel_size=2), nn.ReLU(True))\n",
        "\n",
        "        self.feature = nn.Sequential(self.conv_layer1, self.conv_layer2)\n",
        "    \n",
        "        #### Classifier Layer ####\n",
        "        self.fc1        = nn.Linear(50*7*7, 50, bias=True)\n",
        "        self.fc2        = nn.Linear(50, num_class, bias=True)\n",
        "\n",
        "        self.fc_layer1  = nn.Sequential(self.fc1, nn.ReLU(True))\n",
        "        self.fc_layer2  = nn.Sequential(self.fc2, nn.Sigmoid())\n",
        "\n",
        "        self.classifier = nn.Sequential(self.fc_layer1, self.fc_layer2)\n",
        "        self._initialize_weight() \n",
        "\n",
        "    def _initialize_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        x = x.view(-1, 50*7*7)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1n_k1EIMBnI"
      },
      "source": [
        "## 3. Define functions for Learning\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QgllJaOMF9v"
      },
      "source": [
        "def train(model, data_train, data_train_batch, optimizer, criterion, device='cuda'):\n",
        "    model.train()\n",
        "    n_batch = 0\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    for batch_idx, (x, y) in enumerate(data_train_batch):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model.forward(x)\n",
        "        loss = criterion(pred, y)\n",
        "        avg_loss += loss.item()\n",
        "        avg_acc += accuracy(pred, y)\n",
        "        n_batch += 1\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss /= n_batch\n",
        "    avg_acc /= n_batch\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "def test(model, x, y, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model.forward(x)\n",
        "        loss = criterion(pred, y).item()\n",
        "        acc = accuracy(pred, y)\n",
        "    return loss, acc\n",
        "\n",
        "def accuracy(pred, y):\n",
        "    return 100*(torch.sum(torch.argmax(pred, dim=1)==y)).item()/len(y)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QZD2mvpP2Qu"
      },
      "source": [
        "def learn(model, data_train, data_test, criterion, optimizer, batch_size, epoch, device='cuda'):\n",
        "    data_train_batch = torch.utils.data.DataLoader(data_train, batch_size, shuffle=True)\n",
        "    data_train = data_train.train_data.unsqueeze(1)\n",
        "    test_x = data_test.test_data.unsqueeze(1)\n",
        "    test_y = data_test.test_labels\n",
        "    test_x, test_y = torch.tensor(test_x, dtype=torch.float, device=device), test_y.to(device)\n",
        "    i = 0\n",
        "    while i<epoch:\n",
        "        losstest, acctest = test(model, test_x, test_y, criterion)\n",
        "        losstrain, acctrain = train(model, data_train, data_train_batch, optimizer, criterion, device)\n",
        "        \n",
        "        loss_train.append(losstrain); loss_test.append(losstest)\n",
        "        acc_train.append(acctrain); acc_test.append(acctest)\n",
        "        i += 1\n",
        "        print(len(loss_train),'\\t', losstrain,'\\t', acctrain,'\\t', losstest,'\\t', acctest)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUF7wmQiP5Zm"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if is_cuda else 'cpu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJJoXTrKQSHN"
      },
      "source": [
        "## 4. Learn\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7nr2iFeQRxt",
        "outputId": "5116417f-1701-4fbd-9f83-1f8a0c32a022"
      },
      "source": [
        "global loss_train, loss_test, acc_train, acc_test\n",
        "loss_train = []; loss_test = []; acc_train = []; acc_test = []\n",
        "model = CNN_module().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#criterion = nn.NLLLoss2d()\n",
        "while True:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0015)\n",
        "    learn(model, data_train, data_test, criterion, optimizer, 32, 1, device)\n",
        "    if acc_test[-1] > 95:\n",
        "        break"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 \t 1.6348427469357134 \t 83.37659744408946 \t 2.4106104373931885 \t 9.228333333333333\n",
            "2 \t 1.5131438353572029 \t 95.74680511182109 \t 1.5647214651107788 \t 74.11833333333334\n",
            "3 \t 1.4965402996197295 \t 96.96485623003196 \t 1.5142992734909058 \t 93.44666666666667\n",
            "4 \t 1.4889058221262483 \t 97.81349840255591 \t 1.5016241073608398 \t 93.40166666666667\n",
            "5 \t 1.4837122744264695 \t 98.10303514376997 \t 1.4959627389907837 \t 94.44333333333333\n",
            "6 \t 1.4807250789179207 \t 98.3626198083067 \t 1.493674635887146 \t 94.99\n",
            "7 \t 1.4786829502818684 \t 98.59225239616613 \t 1.491932988166809 \t 95.01333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5JUFzqTRlv0"
      },
      "source": [
        "while True:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0015)\n",
        "    learn(model, data_train, data_test, criterion, optimizer, 64, 1, device)\n",
        "    if acc_test[-1] > 97:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXzubezEcccJ"
      },
      "source": [
        "while True:\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.0015)\n",
        "    learn(model, data_train, data_test, criterion, optimizer, 32, 1, device)\n",
        "    if loss_test[-1] > loss_test[-2] > loss_test[-3] > loss_test[-4] > loss_test[-5]:\n",
        "        for i in range(4):\n",
        "            del loss_test[-1]; del loss_train[-1]; del acc_test[-1]; del acc_train[-1]\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GaoSctsiWVO",
        "outputId": "929d1008-2a47-40d8-a27f-afc4ee6a7ee6"
      },
      "source": [
        "print(acc_test[-1])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96.88\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}